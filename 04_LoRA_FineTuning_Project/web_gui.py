import streamlit as st
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import PeftModel

# è¨­å®šç¶²é æ¨™é¡Œ
st.set_page_config(page_title="åµåµçš„ AI å½±è©•åˆ†æå®¤", page_icon="ğŸ¬")

@st.cache_resource # è®“æ¨¡å‹åªè¼‰å…¥ä¸€æ¬¡ï¼Œé¿å…é‡è¤‡ä½”ç”¨è¨˜æ†¶é«”
def load_model():
    MODEL_NAME = "distilbert-base-uncased"
    LORA_PATH = "./results_lora_ft/final_lora_adapter"
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
    model = PeftModel.from_pretrained(base_model, LORA_PATH).to(DEVICE)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model.eval()
    return model, tokenizer, DEVICE

# åˆå§‹åŒ–
st.title("ğŸ¬ åµåµçš„ AI å½±è©•åˆ†æå®¤")
st.write("è¼¸å…¥ä¸€æ®µé›»å½±è©•è«–ï¼Œè®“æˆ‘çš„ LoRA å¾®èª¿æ¨¡å‹å¹«ä½ åˆ†æå®ƒçš„æƒ…æ„Ÿï¼")

model, tokenizer, DEVICE = load_model()
user_input = st.text_area("è«‹è¼¸å…¥è‹±æ–‡å½±è©•ï¼š", placeholder="Type something like 'What a fantastic movie!'")

if st.button("é–‹å§‹åˆ†æ"):
    if user_input.strip():
        # æ¨ç†é‚è¼¯
        inputs = tokenizer(user_input, return_tensors="pt", truncation=True, padding=True).to(DEVICE)
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            pred_id = torch.argmax(probs, dim=-1).item()
            conf = probs[0][pred_id].item() * 100

        # é¡¯ç¤ºçµæœ
        label_map = {0: ("Negative ğŸ˜”", "red"), 1: ("Positive ğŸ˜Š", "green")}
        label_text, color = label_map[pred_id]
        
        st.subheader(f"é æ¸¬çµæœï¼š:{color}[{label_text}]")
        st.info(f"ä¿¡å¿ƒæŒ‡æ•¸ï¼š{conf:.2f}%")
    else:
        st.warning("è«‹è¨˜å¾—è¼¸å…¥æ–‡å­—å–”ï¼")