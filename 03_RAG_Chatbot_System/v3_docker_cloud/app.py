
import streamlit as st
import os
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
# åŠ å…¥çŸ­æœŸè¨˜æ†¶
from langchain_classic.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain_classic.memory import ConversationBufferMemory

# --- 1. é…ç½®å€ ---
load_dotenv()
DOCUMENTS_PATH = "data/knowledge_sample.pdf"
VECTOR_DB_DIR = "chroma_db"
HF_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
RETRIEVER_TOP_K = 5

# --- 2. ç·©å­˜çµ„ä»¶ ---
@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(model_name=HF_MODEL_NAME)

@st.cache_resource
def setup_knowledge_base():
    embeddings = get_embeddings()
    # å¦‚æœè³‡æ–™åº«å·²å­˜åœ¨å‰‡åŠ è¼‰
    if os.path.exists(VECTOR_DB_DIR):
        return Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embeddings)
    
    # ä¸å­˜åœ¨å‰‡é‡æ–°å»ºç«‹
    if not os.path.exists(DOCUMENTS_PATH):
        return None

    loader = PyPDFLoader(DOCUMENTS_PATH)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = splitter.split_documents(docs)
    return Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=VECTOR_DB_DIR)

def create_qa_chain(vectorstore):
    if "GEMINI_API_KEY" not in os.environ:
        return None
    
    # åˆå§‹åŒ– LLM å’Œæª¢ç´¢å™¨
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
    retriever = vectorstore.as_retriever(search_kwargs={"k": RETRIEVER_TOP_K})

    # åˆå§‹åŒ–çŸ­æœŸè¨˜æ†¶ï¼šé€™æ˜¯ V3 çš„æ ¸å¿ƒäº®é»
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=True,
        verbose=False 
    )

# --- 3. Streamlit ä»‹é¢é‚è¼¯ ---
def clear_history():
    if "messages" in st.session_state: del st.session_state.messages
    if "qa_chain" in st.session_state: del st.session_state.qa_chain

def main():
    st.set_page_config(page_title="Gemini RAG è¨˜æ†¶åŠ©æ‰‹", layout="wide")
    st.title("ğŸš€ Project: é«˜ç´šæ–‡ä»¶å•ç­”æ©Ÿå™¨äºº (V3)")
    st.caption("å…·å‚™å°è©±è¨˜æ†¶åŠŸèƒ½ï¼Œèƒ½ç†è§£ä¸Šä¸‹æ–‡é€£çµã€‚")

    with st.sidebar:
        st.button("ğŸ§¹ æ¸…é™¤å°è©±ç´€éŒ„", on_click=clear_history)
        if "GEMINI_API_KEY" not in os.environ:
            st.error("ğŸš¨ æ‰¾ä¸åˆ° API Keyï¼Œè«‹æª¢æŸ¥ .env")

    # åˆå§‹åŒ–çŸ¥è­˜åº«
    vectorstore = setup_knowledge_base()
    if not vectorstore:
        st.warning(f"è«‹ä¸Šå‚³æ–‡ä»¶è‡³ `{DOCUMENTS_PATH}`")
        st.stop()

    # åˆå§‹åŒ–å°è©±éˆ
    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = create_qa_chain(vectorstore)
    
    qa_chain = st.session_state.qa_chain
    if not qa_chain:
        st.error("ç„¡æ³•å•Ÿå‹•å•ç­”éˆï¼Œè«‹æª¢æŸ¥ API Keyã€‚")
        st.stop()

    # èŠå¤©è¨Šæ¯è™•ç†
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘å·²æº–å‚™å¥½å›ç­”é—œæ–¼æ–‡ä»¶çš„ç´°ç¯€ï¼Œæˆ‘ä¹Ÿæœƒè¨˜å¾—æˆ‘å€‘ä¹‹å‰çš„å°è©±å…§å®¹ã€‚"}]

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸¦æª¢ç´¢ä¸­..."):
                try:
                    # èª¿ç”¨å°è©±éˆ
                    result = qa_chain.invoke({"question": prompt})
                    answer = result["answer"]
                    sources = result.get("source_documents", [])
                    
                    # æ ¼å¼åŒ–è¼¸å‡ºä¾†æº
                    source_info = "\n\n---\n**ğŸ“š è³‡è¨Šä¾†æºï¼š**\n" + "\n".join(
                        [f"- ç¬¬ {doc.metadata.get('page','?')} é " for doc in sources]
                    )
                    
                    full_response = f"{answer}{source_info}"
                    st.markdown(full_response)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                except Exception as e:
                    st.error(f"åŸ·è¡Œå‡ºéŒ¯: {e}")

if __name__ == "__main__":
    main()