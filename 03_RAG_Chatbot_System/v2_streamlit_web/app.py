import streamlit as st
import os
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_classic.chains.retrieval_qa.base import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
# åŠ å…¥çŸ­æœŸè¨˜æ†¶
from langchain_classic.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain_classic.memory import ConversationBufferMemory

# --- 1. åˆå§‹åŒ–èˆ‡é…ç½® ---
load_dotenv()

# è¨­å®šé è¨­æ–‡ä»¶è·¯å¾‘ (èˆ‡ V1 ä¸€è‡´)
DOCUMENTS_PATH = "data/knowledge_sample.pdf"
VECTOR_DB_DIR = "chroma_db"
HF_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# æª¢æŸ¥ API Key
if "GEMINI_API_KEY" not in os.environ:
    st.error("ğŸš¨ æ‰¾ä¸åˆ° GEMINI_API_KEYï¼è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šã€‚")
    st.stop()

# --- 2. ç·©å­˜è³‡æº (é¿å…é‡è¤‡è¼‰å…¥æ¨¡å‹) ---
@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(model_name=HF_MODEL_NAME)

@st.cache_resource
def setup_vectorstore():
    embeddings = get_embeddings()
    # å¦‚æœå·²æœ‰è³‡æ–™åº«å‰‡åŠ è¼‰ï¼Œå¦å‰‡å»ºç«‹
    if os.path.exists(VECTOR_DB_DIR):
        return Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embeddings)
    
    if os.path.exists(DOCUMENTS_PATH):
        loader = PyPDFLoader(DOCUMENTS_PATH)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        texts = text_splitter.split_documents(documents)
        return Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=VECTOR_DB_DIR)
    
    return None

# --- 3. å»ºç«‹å°è©±éˆ (å«è¨˜æ†¶åŠŸèƒ½) ---
def create_chain(vectorstore):
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    
    # åˆå§‹åŒ–çŸ­æœŸè¨˜æ†¶
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=True
    )

# --- 4. Streamlit ä»‹é¢ ---
def main():
    st.set_page_config(page_title="AI æ–‡ä»¶åŠ©æ‰‹", layout="centered")
    st.title("ğŸ¤– ä¼æ¥­ç´š RAG å•ç­”æ©Ÿå™¨äºº")
    st.info("æœ¬æ©Ÿå™¨äººå…·å‚™ã€Œå°è©±è¨˜æ†¶ã€åŠŸèƒ½ï¼Œæ‚¨å¯ä»¥é‡å°å…ˆå‰çš„å›ç­”ç¹¼çºŒè¿½å•ã€‚")

    # åˆå§‹åŒ–çŸ¥è­˜åº«
    vectorstore = setup_vectorstore()
    if not vectorstore:
        st.warning(f"è«‹ç¢ºä¿ `{DOCUMENTS_PATH}` æª”æ¡ˆå­˜åœ¨ã€‚")
        return

    # åˆå§‹åŒ–å°è©±éˆ
    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = create_chain(vectorstore)
    
    # åˆå§‹åŒ–èŠå¤©ç´€éŒ„
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æ–‡ä»¶åŠ©æ‰‹ï¼Œè«‹å•ä»Šå¤©æƒ³äº†è§£ä»€éº¼ï¼Ÿ"}]

    # é¡¯ç¤ºå°è©±æ­·å²
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ä½¿ç”¨è€…è¼¸å…¥
    if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ç”Ÿæˆå›ç­”
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                result = st.session_state.qa_chain.invoke({"question": prompt})
                answer = result["answer"]
                
                # æ•´ç†ä¾†æºè³‡è¨Š
                sources = result.get("source_documents", [])
                source_text = "\n\n**ğŸ“š åƒè€ƒä¾†æºï¼š**\n" + "\n".join([f"- ç¬¬ {doc.metadata.get('page','?')} é " for doc in sources])
                
                full_response = f"{answer}{source_text}"
                st.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()