import re
import spacy
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import json
import ollama
from pathlib import Path
import os

INPUT_DIR = "data/raw_documents"
OUTPUT_DIR = "data/processed_documents"

# å®šç¾©çµæ§‹åŒ– Prompt
ANALYSIS_PROMPT = """
ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ä¿éšªç†è³ å°ˆå®¶èˆ‡æ³•å¾‹åŠ©ç†ã€‚è«‹å¾ä¸‹æ–¹æä¾›çš„ã€Œå»è­˜åˆ¥åŒ–ã€è£åˆ¤æ›¸æ–‡æœ¬ä¸­ï¼Œæå–é—œæ–¼ã€Œç²¾ç¥æ…°æ’«é‡‘ã€çš„è³‡è¨Šã€‚

è«‹éµå¾ªä»¥ä¸‹é‚è¼¯ï¼š
1. é‡‘é¡æå–ï¼šè«‹è¨ˆç®—æ³•é™¢æœ€çµ‚æ ¸å®šçš„ã€Œç²¾ç¥æ…°æ’«é‡‘ç¸½é¡ã€ã€‚
2. ç†ç”±ç¸½çµï¼šè«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡è¿°æ³•é™¢æ ¸å®šæ­¤é‡‘é¡çš„å…·é«”å› ç´ ï¼ˆå¦‚ï¼šéå¤±ç¨‹åº¦ã€å—å®³äººå¹´é½¡ã€å®¶å±¬ç—›è‹¦ç¨‹åº¦ã€é›™æ–¹ç¶“æ¿Ÿåœ°ä½ï¼‰ã€‚
3. æ ¼å¼ï¼šå‹™å¿…åƒ…å›å‚³ JSON æ ¼å¼ã€‚

{{
    "amount": æ•¸å­—,
    "reason_summary": "ä¸­æ–‡ç†ç”±ç¸½çµ",
    "case_id": "è£åˆ¤å­—è™Ÿ"
}}

æ–‡æœ¬å…§å®¹ï¼š
{text}
"""

class InsureDataProcessor:
    def __init__(self, whitelist=None):
        try:
            self.nlp = spacy.load("zh_core_web_trf")
        except:
            print("è«‹å…ˆåŸ·è¡Œ python -m spacy download zh_core_web_trf")

        self.role_pattern = r"(ä¸Š\s*è¨´\s*äºº|è¢«\s*ä¸Š\s*è¨´\s*äºº|è¨´è¨Ÿä»£ç†äºº|æ³•å®šä»£ç†äºº|åŸ\s*å‘Š|è¢«\s*å‘Š)"
        self.whitelist = whitelist if whitelist else {"å¾‹å¸«", "æ³•å®˜", "æ›¸è¨˜å®˜", "ä¸Šä¸€äºº", "å…±åŒ"}
        # ç”¨æ–¼åˆ‡åˆ†è£åˆ¤æ›¸å¤§æ®µè½çš„æ¨™è¨˜
        self.section_markers = {
            "main_judgment": r"\n\s*ä¸»\s*æ–‡\s*\n",
            "facts_and_reasons": r"\n\s*äº‹å¯¦åŠç†ç”±\s*\n",
            "conclusion_start": r"\n\s*ä¸­\s*è¯\s*æ°‘\s*åœ‹.*\n"
        }

    def split_sections(self, text):
        """å°‡è£åˆ¤æ›¸åˆ‡åˆ†ç‚ºï¼šHeader, ä¸»æ–‡, äº‹å¯¦åŠç†ç”±, Footer"""
        sections = {}
        
        # å°‹æ‰¾åˆ†éš”é»
        main_match = re.search(self.section_markers["main_judgment"], text)
        facts_match = re.search(self.section_markers["facts_and_reasons"], text)
        remain_match = re.search(self.section_markers["conclusion_start"], text)
        
        if main_match and facts_match:
            sections["header"] = text[:main_match.start()]
            sections["judgment"] = text[main_match.end():facts_match.start()]
            sections["content"] = text[facts_match.end():remain_match.start()]
            sections["footer"] = text[remain_match.end():]
        else:
            sections["full_text"] = text
        return sections
    
    def get_metadata(self, text):
        """æå–è£åˆ¤æ›¸å…ƒæ•¸æ“š"""
        case_id = self.extract_case_id(text)
        return {
            "case_id": case_id,
            "source": "æ³•é™¢åˆ¤æ±ºæ›¸"
        }

    def extract_case_id(self, text):
        pattern = r"(\d+\s*å¹´\s*[\u4e00-\u9fa5]+\s*å­—ç¬¬\s*\d+\s*è™Ÿ)"
        match = re.search(pattern, text)
        return match.group(1).replace(" ", "") if match else "Unknown_ID"
    
    def clean_text(self, text):
        """åŸºç¤æ–‡æœ¬æ¨™æº–åŒ–"""
        return text.replace("ã€€", " ").strip()
    
    def extract_target_names(self, text):
        """
        å¾è£åˆ¤æ›¸æŠ¬é ­æå–é—œéµäººç‰©å§“åï¼Œå°æ¨™é‡‘èæ•¸æ“šå»è­˜åˆ¥åŒ–åˆè¦è¦æ±‚ã€‚
        """
        text = self.clean_text(text)
        target_names = set()
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        for i, line in enumerate(lines):
            match = re.search(self.role_pattern, line)
            if match:
                role_text = match.group(0)
                # ç²å–è§’è‰²é—œéµå­—å¾Œçš„å…§å®¹
                raw_candidate = re.sub(f".*{role_text}", "", line).strip()
                
                # å¦‚æœè©²è¡Œæ²’åå­—ï¼Œçœ‹ä¸‹ä¸€è¡Œï¼ˆè™•ç†æ ¼å¼ç¸®æ’å•é¡Œï¼‰
                if len(raw_candidate) < 2 and i + 1 < len(lines):
                    next_line = lines[i+1].strip()
                    if not re.search(self.role_pattern, next_line):
                        raw_candidate = next_line
                
                # åŸ·è¡Œæ·±åº¦æ¸…æ´—
                final_name = self._refine_name(raw_candidate)
                if final_name:
                    target_names.add(final_name)
        
        return list(target_names)
    
    def _refine_name(self, name):
        for word in self.whitelist:
            name = name.replace(word, "")
        name = re.sub(r"[^\u4e00-\u9fa5]", "", name)
        if 2 <= len(name) <= 4:
            return name
        return None
       
    def nlp_refine_names(self, text):
        """
        åˆ©ç”¨ NLP åµæ¸¬æ–‡ä¸­æ½›è—çš„äººåï¼ˆä¾‹å¦‚ï¼šæ—ä½•æœˆå¨¥ã€é™³å»ºç‘‹ï¼‰ã€‚
        """
        doc = self.nlp(text)
        extra_names = set()
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                name = ent.text
                # éæ¿¾æ‰å¤ªçŸ­æˆ–åœ¨ç™½åå–®å…§çš„è©
                if len(name) >= 2 and name not in self.whitelist:
                    extra_names.add(name)
        return extra_names

    def mask_text(self, text, target_names):
        """
        å»è­˜åˆ¥åŒ–é®è”½ï¼š
        1. åˆä½µåå–®ä¸¦å»é‡ã€‚
        2. éæ¿¾æ‰é•·åº¦ä¸è¶³æˆ–å±¬æ–¼å­å­—ä¸²çš„é›œè¨Šã€‚
        3. ä½¿ç”¨æ­£å‰‡ä¸€æ¬¡æ€§æ›¿æ›ï¼Œé¿å…é‡è¤‡é®è”½ã€‚
        """
        # å–å¾— NLP åµæ¸¬åå–®
        nlp_names = self.nlp_refine_names(text)
        
        # åˆä½µä¸¦æ¸…ç†ï¼šåªä¿ç•™é•·åº¦ >= 2 çš„åç¨±
        combined_names = set(n for n in (set(target_names) | nlp_names) if len(n) >= 2)
        
        # éæ¿¾å­å­—ä¸²ï¼Œé¿å…é‡è¤‡é®è”½
        sorted_names = sorted(list(combined_names), key=len, reverse=True)
        final_list = []
        for name in sorted_names:
            if not any(name in existing and name != existing for existing in final_list):
                final_list.append(name)
        
        masked_text = text
        # ç”±é•·åˆ°çŸ­æ›¿æ›
        for name in final_list:
            # å§“å -> å§“ + ã€‡ (+ ã€‡)
            mask = name[0] + "ã€‡" * (len(name) - 1)
            masked_text = masked_text.replace(name, mask)
            
        return masked_text, final_list


class InsureAnalysisEngine:
    def __init__(self, db_path="./chroma_db"):
        # 1. åˆå§‹åŒ– Embedding æ¨¡å‹ 
        self.embedding_model = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="BAAI/bge-large-zh-v1.5"
        )
        
        # 2. åˆå§‹åŒ– ChromaDB
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection(
            name="insure_cases",
            embedding_function=self.embedding_model
        )

    def extract_features(self, masked_text):
        """
        æ¨¡æ“¬å‘¼å« Llama 3 é€²è¡Œçµæ§‹åŒ–æå–ã€‚
        å¯¦éš›æ“ä½œæ™‚å¯ä¸²æ¥ Ollama API æˆ– Groqã€‚
        """
        response = ollama.generate(
            model='llama3', 
            prompt=ANALYSIS_PROMPT.format(text=masked_text),
            format='json',
            options={"temperature": 0} # è¨­å®šç‚º 0 å¢åŠ è§£æç©©å®šæ€§
        )
        
        try:
            data = json.loads(response['response'])
            return data
        except Exception as e:
            print(f"è§£æ JSON å‡ºéŒ¯: {e}")
            return None

    def upsert_to_vector_db(self, case_data, masked_text):
        """
        å°‡ç‰¹å¾µèˆ‡æ–‡æœ¬å­˜å…¥ ChromaDB
        """
        self.collection.add(
            documents=[masked_text], # å­˜å…¥å»è­˜åˆ¥åŒ–å¾Œçš„æ–‡æœ¬ä¾›æœªä¾† RAG æª¢ç´¢
            metadatas=[{
                "case_id": case_data["case_id"],
                "final_amount": case_data["amount"],
                "reason": case_data["reason_summary"]
            }],
            ids=[case_data["case_id"]]
        )
        print(f"âœ… æ¡ˆä»¶ {case_data['case_id']} å·²æˆåŠŸç´¢å¼•è‡³å‘é‡è³‡æ–™åº«ã€‚")
    
    def query_similar_cases(self, current_case_text, current_amount, n_results=3):
        """
        [RAG æª¢ç´¢èˆ‡é¢¨éšªé è­¦]
        1. æ ¹æ“šç›®å‰æ¡ˆæƒ…ï¼Œæª¢ç´¢ç›¸ä¼¼åˆ¤ä¾‹
        2. è¨ˆç®—ç›®å‰ç”³è«‹é‡‘é¡æ˜¯å¦åé›¢æ­·å²å¸‚å ´è¡Œæƒ…
        """
        # æŸ¥è©¢å‘é‡è³‡æ–™åº«
        results = self.collection.query(
            query_texts=[current_case_text],
            n_results=n_results
        )

        # æå–æ­·å²é‡‘é¡é€²è¡Œé¢¨éšªè©•ä¼°
        history_amounts = [m["final_amount"] for m in results["metadatas"][0]]
        avg_amount = sum(history_amounts) / len(history_amounts) if history_amounts else 0
        
        # é¢¨éšªè¨ˆç®—ï¼š(ç›®å‰é‡‘é¡ - æ­·å²å¹³å‡) / æ­·å²å¹³å‡
        risk_score = (current_amount - avg_amount) / avg_amount if avg_amount > 0 else 0
        is_high_risk = risk_score > 0.3  # åé›¢ 30% å³æ¨™è¨˜é«˜é¢¨éšª

        return {
            "similar_cases": results["metadatas"][0],
            "market_average": avg_amount,
            "deviation_ratio": f"{risk_score:.2%}",
            "risk_alert": "ğŸ”´ é«˜é¢¨éšª - è«‹æ±‚é‡‘é¡é¡¯è‘—é«˜æ–¼æ­·å²åˆ¤ä¾‹" if is_high_risk else "ğŸŸ¢ æ­£å¸¸ - ç¬¦åˆå¸‚å ´è¡Œæƒ…"
        }

def load_text_file(file_path):
    """
    æª”æ¡ˆè®€å–
    """
    path = Path(file_path)
    if not path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{file_path}")
    
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def main():
    # 1. æª”æ¡ˆè®€å– 
    try:
        file_name = "text1.txt" 
        raw_text = load_text_file(os.path.join(INPUT_DIR, file_name))
        print(f"æˆåŠŸè®€å–æª”æ¡ˆ: {file_name}")
    except Exception as e:
        print(f"æª”æ¡ˆè®€å–å¤±æ•—: {e}")
        return
    
    # 2. åˆå§‹åŒ–è™•ç†å™¨
    processor = InsureDataProcessor()

    # 3. æ•¸æ“šç®¡ç·šï¼šåˆ‡åˆ† -> æå–åå–® -> å»è­˜åˆ¥åŒ–
    sections = processor.split_sections(raw_text)
    header_names = processor.extract_target_names(sections.get("header", ""))
    metadata = processor.get_metadata(sections.get("header", ""))
    case_id = metadata["case_id"]

    content = sections.get("content", "")
    safe_content, detected_all = processor.mask_text(content, header_names)
    print(safe_content)

    analyzer = InsureAnalysisEngine()

    # 4. åˆ†æå¼•æ“ï¼šLLM çµæ§‹åŒ–æå–
    features = analyzer.extract_features(safe_content)
    
    if features:
        features["case_id"] = case_id 
        print(f"LLM æå–çµæœ: {json.dumps(features, ensure_ascii=False, indent=2)}")
        print(features)
        
        # 5. å­˜å…¥å‘é‡è³‡æ–™åº«
        analyzer.upsert_to_vector_db(features, safe_content)

        # 6. é¢¨éšªé è­¦æ¨¡æ“¬
        new_claim_amount = 2000000
        new_claim_context = "è¡Œäººèµ°åœ¨æ–‘é¦¬ç·šä¸Šé­å°è²¨è»Šæ’æ“Šè‡´æ­»ï¼Œå®¶å±¬æ¥µåº¦ç—›è‹¦ã€‚"
        risk_report = analyzer.query_similar_cases(new_claim_context, new_claim_amount)

        print("\n=== ç›¸ä¼¼æ¡ˆä¾‹é¢¨éšªè©•ä¼°å ±å‘Š ===")
        print(f"ç›®å‰è«‹æ±‚é‡‘é¡: {new_claim_amount}")
        print(f"æ­·å²ç›¸ä¼¼æ¡ˆå¹³å‡åˆ¤è³ : {risk_report['market_average']}")
        print(f"åé›¢æ¯”ä¾‹: {risk_report['deviation_ratio']}")
        print(f"ç³»çµ±é è­¦: {risk_report['risk_alert']}")

        os.makedirs(OUTPUT_DIR, exist_ok=True)

if __name__ == "__main__":
    main()